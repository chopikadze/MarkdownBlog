Why engineering systems work almost without any errors, but software has bugs usually? What is a difference? What we should steal from engineering disciplines to make software robust as engineer devices? 

<small>
Side note: Does anybody know something good to read about this? I am reading “Lean Startup” now, and Eric Ries notices that “lean manufacturing” from Toyota was the basis of his idea. After this book, I will read about lean manufacturing for sure, but... Is there anything else to read? 
</small>

3 years ago, we bought a new house. House was empty – just foundation, walls and roof. It had only single lamp in center of each room, and single socket per room. Everything was in horrible state. No water, no sewerage, no ventilation – nothing at all. Everything was done from the scratch. The hardest parts were electricity and plumbing. I liked and I like to do it. You have a set of components/blocks. For instance, you have water pipes, pipe elbows, T-branches, threaded connections, boiler, filter etc. For electricity – wires, sockets, switches, circuit breakers etc. You connect everything together – and everything works! Without errors! Yes, you can have a problem, if you pass too high or too low voltage to input. But you can solve it with voltage relay. In result, if your system gets wrong “input data” (voltage) – it just turns off. 

Let’s talk about programming now. For instance, C#. We have something like 50-60 reserved words + 30 “characters” (brackets, plus, minus etc.) As result – something like 100 “blocks”. It’s quite close to a set of blocks for electricity or plumbing. Than why software has bugs but plumbing doesn’t? OK, but usually software is much more complex than plumbing in a house. Maybe. But if we will try to compare electric grid of whole country? I think it’s not simpler than OS even… 

Another example – CPU. Modern CPUs have from 1.5 (Core i7-3770K) to 3 billions (NVidia Fermi/GT300) of transistors. They have from ~200 (i7) to 512 pipelines (Fermi). They predicts next possible instructions (even if they don’t have yet data from RAM), execute them, and if prediction was right – they return result almost immediately. By the way, this prediction block was removed from Atom, and it is one of the reason why it’s quite slower. And CPUs don’t contain bugs. They don’t have patches, they don’t have service packs. Once manufactured CPU works for tens years… Why? 

I think that some reasons could be: 

* Very detailed specification of input and output data. Each “block” of such system needs input data in exact range (e.g., 220V +- 10%), and returns output data in exact range also (e.g. 5V +-5%). 
* Engineering systems are tested scrupulously. Very scrupulously. If we will use development terms, engineering samples have “100% code coverage”, and each “line of code” is covered not just once. They have unit tests, integration tests and much more. 
* Once specification is developed – it doesn’t change. It doesn’t change after start of manufacturing – for sure! Nobody asks to move elevator shaft one meter to the left (it’s just a little!) 
* Nobody except certified specialists could interfere in engineering system. You cannot just come from outside and connect your power plant/factory/ultra-modern-quantum-transformer to power grid of country. For obvious reasons.
* Very often developers use “unnatural” way to create “dependencies” – inside object that needs another object. It looks… quite strange when fridge (that needs 220V) builds power generator near itself, and this plant builds small gasoline pipe to closest gas station. No, in real world everything is not like this. In real world YOU buy fridge, power generator and can with gas, then you connect fridge to power generator and pour gas in power generator. Or you buy sockets/switches/lamps/circuit breakers and then connect everything together with wires. 

I understand that if you add all these stuffs in development process – speed of this development will decrease in times (probably, even in tens times). And probably, these principles are used in development of critical system (like power plants systems or artificial heart valves). But there should be some analogs that you can use in “cheap” way, without significant increasing of development time (you can read – cost of development). 

All stuffs below is regarding C#, but I’m sure that almost all object oriented (and other) languages should have something similar. 

* Clean specification of input and output data. And checking of these constraints. In programming it is – checking of range of input data, processing of boundary conditions etc. I think the closest analog is “Design by Contract” proposed by Bertrand Meyer in Eiffel. Many languages have libraries that provides close functionality (Code Contracts for .NET, Contracts for Java/jContracts for Java etc.) Ideally these libraries should contain two functions (and they usually do) – checking of input data in runtime (as voltage relay) and static checking (e.g. if method returns value in range of 5..10 then obviously it’s unsafe to pass this value to another method than needs input data in range of 7..15, and it’s obviously wrong to pass this value to method that needs input value in range of 20..30). Unfortunately current implementation of Code Contracts from Microsoft has huge con that prevents its usage in production – it adds too big overtime to compilation. But idea is cool and very useful. And I’m sure that development of static analyzers will add a lot of benefits to usage of contracts. 
* Testing. Everything is obvious here. More (proper) tests is better. But it’s a lot of time. And money. As result you should always look for compromise. Is it critical if your DVD player will hang once per year and you have to reboot it? And is it critical if it’s nuclear power plant? Next step in testing is automation. Human’s time costs much more than computer’s one, and this is why you should have automated tests as much as possible, because more automated tests you have – it’s cheaper to execute these tests. But you should write these tests, and it increases the cost of development. Again – you should always look for compromise. Unit tests are obvious but when you move higher in hierarchy you see more and more problems. How to test layer that works with database? How can you emulate internet connection? How to abstract from database at all? How to test UI? And I think if you will investigate these questions – it will increase your code quality a lot. Dependency Injection (I will talk about it later) and MVC/MVVM/etc. patterns help a lot exactly with testing. Test Driven Development, Continuous Integration… And if there is still discussion regarding TDD (it’s especially cool when you develop math libraries), then regarding CI everything is clear. You don’t use CI? Well, it’s sadly… 
* Specification doesn’t change after creating. Well, here are too many opinions. Somebody uses waterfall (and it’s perfect when you need big product with good quality), somebody uses Agile (I’m not sure if “specification” exists at all in Agile). And, you know – both are right! But more closer you are to Agile – more automated tests you should have. Otherwise you will find yourself in fixing hell very soon. 
* Nobody except certified specialists could interfere in engineering system. Abstraction will help a lot on library/application level. For OS – you should also separate everything in layers without possibility of easy access to level below (e.g. kernel level). Separation of processes (so one process could not use e.g. memory of another one) helps a lot too. 
* Inversion of Control, proposed by Martin Fowler in the far 1988, absolutely wrong implementation with Service Locator, and much better implementation with Dependency Injection. Dependency Injection is really proper way to connect “blocks”. In example with fridge – fridge doesn’t care who will be provider of electricity. It could be home electric grid, it could be offline power generator. Fridge just needs something that will implement interface “IElectricityFactory”. Power generator doesn’t care who will provide gas – man, gasoline pipeline or gas tank. It needs just something that will implement interface “IGasFactory”. So, we have 2 independent classes – “Fridge” and “PowerGenerator”. Then man (Dependency Injection Container) appears and decides to connect power generator as dependency to fridge, and decides that can will “implement” “IGasFactory”. If you will have two fridges – you can use two independent “ElectricityFactories” or use the same – you are DI Container, you decide, and not fridge. 
* There are flying cars, but usual car is better for movement on roads, and plane is better for flying. So – split, split, split. Each class or service should be responsible for single area. Ideally – it should be single. In reality, it’s often 2-3. Max – 5. Yes, short-term memory can hold up to 7 objects usually, but you cannot hold more than 3 areas of responsibility for single class in head. 

In conclusion, here is brief list below where I’m trying to summarize everything: 

* Continuous Integration. Must have. 
* TDD where it could be applicable. For math/processing libraries – must have. 
* Reasonable test coverage. 
* Dependency Injection. Once you will lost in your system. You will forget one-half of dependencies, but –you will maintain your system without big problems, believe me. 
* Services. Use them as layers between applications, use them as layers in your application inside. Use them everywhere. 
* Split, split, split. 